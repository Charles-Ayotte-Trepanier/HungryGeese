{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012469,
     "end_time": "2021-02-28T13:41:35.586039",
     "exception": false,
     "start_time": "2021-02-28T13:41:35.573570",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Purpose of this notebook\n",
    "\n",
    "This notebook is just a proof of concept on how to build a simple q-learning agent, so far it doesn't do great (it's improved from the start but still, no winner) but I'll try my best to improve it, if not at least I save someone some ground work... ;-)\n",
    "\n",
    "\n",
    "## Basic QLearner\n",
    "\n",
    "\n",
    "Equation used to update q-table can be seen on image:\n",
    "\n",
    "![Q-Learning](https://wikimedia.org/api/rest_v1/media/math/render/svg/678cb558a9d59c33ef4810c9618baf34a9577686) Source: Wikipedia\n",
    "\n",
    "\n",
    "\n",
    "### Epsilon greedy policy\n",
    "\n",
    "The epsilon greedy policy means at each step we choose the action that yields a higher Q value (estimated reward) from the current state, but a certain epsilon-percent of times we chose a random action (to explore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-28T13:41:35.625768Z",
     "iopub.status.busy": "2021-02-28T13:41:35.625072Z",
     "iopub.status.idle": "2021-02-28T13:41:35.628415Z",
     "shell.execute_reply": "2021-02-28T13:41:35.627634Z"
    },
    "papermill": {
     "duration": 0.0314,
     "end_time": "2021-02-28T13:41:35.628649",
     "exception": false,
     "start_time": "2021-02-28T13:41:35.597249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random as rand\n",
    "import pickle\n",
    "\n",
    "#This class encapsulates a simple qlearning with epsilon-greedy policy, the states and transitions can be defined automatically as we explore (search space is too big to initialize all at once and many states won't be achievable)\n",
    "class QLearner():\n",
    "    def __init__(self, actions, states=None, initial_value=0.1, alpha=0.05, gamma=0.8, epsilon=0.1, create_states_on_exploration=True):\n",
    "        self.actions = actions\n",
    "        self.create_states_on_exploration = create_states_on_exploration\n",
    "        self.initial_value = initial_value\n",
    "        if states!=None:\n",
    "            self.q_table = {\n",
    "                state: [initial_value for _ in self.actions] for state in states\n",
    "            }\n",
    "            self.states = states\n",
    "        else:\n",
    "            self.q_table = dict()\n",
    "            self.states = []\n",
    "            \n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.previous_state = None\n",
    "        self.current_state = None\n",
    "        self.last_action = None\n",
    "        self.last_action_index = None\n",
    "\n",
    "        \n",
    "    def _check_auto_init_state(self, state):\n",
    "        if (state!=None) and (state not in self.q_table.keys()) and self.create_states_on_exploration:\n",
    "            self.q_table[state] = [self.initial_value for _ in self.actions]\n",
    "            self.states.append(state)\n",
    "    \n",
    "\n",
    "    def _epsilon_greedy(self, state):\n",
    "        #create state if needed\n",
    "        self._check_auto_init_state(state)\n",
    "        \n",
    "        if (rand.random() < self.epsilon):\n",
    "            action = rand.choice(self.actions)\n",
    "            self.last_action_index = self.actions.index(action)\n",
    "        else:\n",
    "            q_state = self.q_table[state]\n",
    "            max_val = max(q_state)\n",
    "            self.last_action_index = rand.choice([i for i,v in enumerate(q_state) if v==max_val])\n",
    "            action = self.actions[self.last_action_index]\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def process_reward(self, reward, previous_state=None, last_action=None, last_action_index=None):\n",
    "        if previous_state==None:\n",
    "            previous_state = self.previous_state\n",
    "        if last_action==None:\n",
    "            last_action = self.last_action\n",
    "        if last_action_index==None:\n",
    "            last_action_index = self.last_action_index\n",
    "            \n",
    "        if (previous_state==None) or (last_action==None):\n",
    "            return\n",
    "        \n",
    "        #create state if needed\n",
    "        self._check_auto_init_state(previous_state)\n",
    "        \n",
    "        q = self.q_table\n",
    "        q_old = q[previous_state][last_action_index]\n",
    "        next_state = self.current_state\n",
    "        if next_state!=None:        \n",
    "            best_scenario = q[next_state].index(max(q[next_state]))\n",
    "            q[previous_state][last_action_index] = q_old + self.alpha * (reward + self.gamma * best_scenario - q_old)\n",
    "        else:\n",
    "            q[previous_state][last_action_index] = q_old + self.alpha * (reward + self.initial_value - q_old)\n",
    "\n",
    "            \n",
    "    def epsilon_greedy_choose_action(self, state):\n",
    "        self.previous_state = self.current_state\n",
    "        self.current_state = state\n",
    "        self.last_action = self._epsilon_greedy(state)\n",
    "        return self.last_action\n",
    "    \n",
    "    \n",
    "    def reset_internal_states(self):\n",
    "        self.previous_state = None\n",
    "        self.current_state = None\n",
    "        self.last_action = None\n",
    "        self.last_action_index = None\n",
    "          \n",
    "            \n",
    "    def save_pickle(self, name):\n",
    "        save_data = (self.actions,\n",
    "                     self.q_table,\n",
    "                     self.states,\n",
    "                     )\n",
    "        with open(f'{name}', 'wb') as handle:\n",
    "            pickle.dump(save_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            \n",
    "    def load_pickle(self, name):\n",
    "        with open(f'{name}', 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "            self.actions, self.q_table, self.states = data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.010998,
     "end_time": "2021-02-28T13:41:35.651369",
     "exception": false,
     "start_time": "2021-02-28T13:41:35.640371",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Hungry Geese - Basic Agent Template Class (greedy risk averse)\n",
    "\n",
    "I'm using the template class I've shared before as a starting point to avoid rewriting and keeping things clear on the qlearning agent (also it's my testing opponent).\n",
    "\n",
    "NOTE:\n",
    "* The code that writes the greedy-goose.py is the same with the adition of the singlenton method (check: https://www.kaggle.com/victordelafuente/hungry-geese-basic-agent-template-class). Here is hidden just for clarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-02-28T13:41:35.707895Z",
     "iopub.status.busy": "2021-02-28T13:41:35.707108Z",
     "iopub.status.idle": "2021-02-28T13:41:35.809390Z",
     "shell.execute_reply": "2021-02-28T13:41:35.808810Z"
    },
    "papermill": {
     "duration": 0.146713,
     "end_time": "2021-02-28T13:41:35.809560",
     "exception": false,
     "start_time": "2021-02-28T13:41:35.662847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading environment football failed: No module named 'gfootball'\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, translate, adjacent_positions, min_distance\n",
    "import random as rand\n",
    "from enum import Enum, auto\n",
    "\n",
    "\n",
    "def opposite(action):\n",
    "    if action == Action.NORTH:\n",
    "        return Action.SOUTH\n",
    "    if action == Action.SOUTH:\n",
    "        return Action.NORTH\n",
    "    if action == Action.EAST:\n",
    "        return Action.WEST\n",
    "    if action == Action.WEST:\n",
    "        return Action.EAST\n",
    "    raise TypeError(str(action) + \" is not a valid Action.\")\n",
    "\n",
    "    \n",
    "\n",
    "#Enconding of cell content to build states from observations\n",
    "class CellState(Enum):\n",
    "    EMPTY = 0\n",
    "    FOOD = auto()\n",
    "    GOOSE = auto()\n",
    "\n",
    "\n",
    "#This class encapsulates mos of the low level Hugry Geese stuff    \n",
    "class BornToNotMedalv2:    \n",
    "    def __init__(self):\n",
    "        self.DEBUG=True\n",
    "        self.rows, self.columns = -1, -1        \n",
    "        self.my_index = -1\n",
    "        self.my_head, self.my_tail = -1, -1\n",
    "        self.geese = []\n",
    "        self.heads = []\n",
    "        self.tails = []\n",
    "        self.food = []\n",
    "        self.cell_states = []\n",
    "        self.actions = [action for action in Action]\n",
    "        self.previous_action = None\n",
    "        self.step = 1\n",
    "\n",
    "        \n",
    "    def _adjacent_positions(self, position):\n",
    "        return adjacent_positions(position, self.columns, self.rows)\n",
    " \n",
    "\n",
    "    def _min_distance_to_food(self, position, food=None):\n",
    "        food = food if food!=None else self.food\n",
    "        return min_distance(position, food, self.columns)\n",
    "\n",
    "    \n",
    "    def _row_col(self, position):\n",
    "        return row_col(position, self.columns)\n",
    "    \n",
    "    \n",
    "    def _translate(self, position, direction):\n",
    "        return translate(position, direction, self.columns, self.rows)\n",
    "        \n",
    "        \n",
    "    def preprocess_env(self, observation, configuration):\n",
    "        observation = Observation(observation)\n",
    "        configuration = Configuration(configuration)\n",
    "        \n",
    "        self.rows, self.columns = configuration.rows, configuration.columns        \n",
    "        self.my_index = observation.index\n",
    "        self.hunger_rate = configuration.hunger_rate\n",
    "        self.min_food = configuration.min_food\n",
    "\n",
    "        self.my_head, self.my_tail = observation.geese[self.my_index][0], observation.geese[self.my_index][-1]        \n",
    "        self.my_body = [pos for pos in observation.geese[self.my_index][1:-1]]\n",
    "\n",
    "        \n",
    "        self.geese = [g for i,g in enumerate(observation.geese) if i!=self.my_index  and len(g) > 0]\n",
    "        self.geese_cells = [pos for g in self.geese for pos in g if len(g) > 0]\n",
    "        \n",
    "        self.occupied = [p for p in self.geese_cells]\n",
    "        self.occupied.extend([p for p in observation.geese[self.my_index]])\n",
    "        \n",
    "        \n",
    "        self.heads = [g[0] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n",
    "        self.bodies = [pos  for i,g in enumerate(observation.geese) for pos in g[1:-1] if i!=self.my_index and len(g) > 2]\n",
    "        self.tails = [g[-1] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 1]\n",
    "        self.food = [f for f in observation.food]\n",
    "        \n",
    "        self.adjacent_to_heads = [pos for head in self.heads for pos in self._adjacent_positions(head)]\n",
    "        self.adjacent_to_bodies = [pos for body in self.bodies for pos in self._adjacent_positions(body)]\n",
    "        self.adjacent_to_tails = [pos for tail in self.tails for pos in self._adjacent_positions(tail)]\n",
    "        self.adjacent_to_geese = self.adjacent_to_heads + self.adjacent_to_bodies\n",
    "        self.danger_zone = self.adjacent_to_geese\n",
    "        \n",
    "        #Cell occupation\n",
    "        self.cell_states = [CellState.EMPTY.value for _ in range(self.rows*self.columns)]\n",
    "        for g in self.geese:\n",
    "            for pos in g:\n",
    "                self.cell_states[pos] = CellState.GOOSE.value\n",
    "        for pos in self.heads:\n",
    "                self.cell_states[pos] = CellState.GOOSE.value\n",
    "        for pos in self.my_body:\n",
    "            self.cell_states[pos] = CellState.GOOSE.value\n",
    "                \n",
    "        #detect dead-ends\n",
    "        self.dead_ends = []\n",
    "        for pos_i,_ in enumerate(self.cell_states):\n",
    "            if self.cell_states[pos_i] != CellState.EMPTY.value:\n",
    "                continue\n",
    "            adjacent = self._adjacent_positions(pos_i)\n",
    "            adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n",
    "            num_blocked = sum(adjacent_states)\n",
    "            if num_blocked>=(CellState.GOOSE.value*3):\n",
    "                self.dead_ends.append(pos_i)\n",
    "        \n",
    "        #check for extended dead-ends\n",
    "        new_dead_ends = [pos for pos in self.dead_ends]\n",
    "        while new_dead_ends!=[]:\n",
    "            for pos in new_dead_ends:\n",
    "                self.cell_states[pos]=CellState.GOOSE.value\n",
    "                self.dead_ends.append(pos)\n",
    "            \n",
    "            new_dead_ends = []\n",
    "            for pos_i,_ in enumerate(self.cell_states):\n",
    "                if self.cell_states[pos_i] != CellState.EMPTY.value:\n",
    "                    continue\n",
    "                adjacent = self._adjacent_positions(pos_i)\n",
    "                adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n",
    "                num_blocked = sum(adjacent_states)\n",
    "                if num_blocked>=(CellState.GOOSE.value*3):\n",
    "                    new_dead_ends.append(pos_i)                                    \n",
    "        \n",
    "                \n",
    "    def strategy_random(self, observation, configuration):\n",
    "        if self.previous_action!=None:\n",
    "            action = rand.choice([action for action in Action if action!=opposite(self.previous_action)])\n",
    "        else:\n",
    "            action = rand.choice([action for action in Action])\n",
    "        self.previous_action = action\n",
    "        return action.name\n",
    "                        \n",
    "                        \n",
    "    def safe_position(self, future_position):\n",
    "        return (future_position not in self.occupied) and (future_position not in self.adjacent_to_heads) and (future_position not in self.dead_ends)\n",
    "    \n",
    "    \n",
    "    def valid_position(self, future_position):\n",
    "        return (future_position not in self.occupied) and (future_position not in self.dead_ends)    \n",
    "\n",
    "    \n",
    "    def free_position(self, future_position):\n",
    "        return (future_position not in self.occupied) \n",
    "    \n",
    "                        \n",
    "    def strategy_random_avoid_collision(self, observation, configuration):\n",
    "        dead_end_cell = False\n",
    "        free_cell = True\n",
    "        actions = [action \n",
    "                   for action in Action \n",
    "                   for future_position in [self._translate(self.my_head, action)]\n",
    "                   if self.valid_position(future_position)] \n",
    "        if self.previous_action!=None:\n",
    "            actions = [action for action in actions if action!=opposite(self.previous_action)] \n",
    "        if actions==[]:\n",
    "            dead_end_cell = True\n",
    "            actions = [action \n",
    "                       for action in Action \n",
    "                       for future_position in [self._translate(self.my_head, action)]\n",
    "                       if self.free_position(future_position)]\n",
    "            if self.previous_action!=None:\n",
    "                actions = [action for action in actions if action!=opposite(self.previous_action)] \n",
    "            #no alternatives\n",
    "            if actions==[]:\n",
    "                free_cell = False\n",
    "                actions = self.actions if self.previous_action==None else [action for action in self.actions if action!=opposite(self.previous_action)] \n",
    "\n",
    "        action = rand.choice(actions)\n",
    "        self.previous_action = action\n",
    "        if self.DEBUG:\n",
    "            aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n",
    "            dead_ends = \"\" if not dead_end_cell else f', dead_ends={[self._row_col(p1) for p1 in self.dead_ends]}, occupied={[self._row_col(p2) for p2 in self.occupied]}'\n",
    "            if free_cell:\n",
    "                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} dead_end={dead_end_cell}{dead_ends}', flush=True)\n",
    "            else:\n",
    "                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} free_cell={free_cell}', flush=True)\n",
    "        return action.name\n",
    "    \n",
    "    \n",
    "    def strategy_greedy_avoid_risk(self, observation, configuration):        \n",
    "        actions = {  \n",
    "            action: self._min_distance_to_food(future_position)\n",
    "            for action in Action \n",
    "            for future_position in [self._translate(self.my_head, action)]\n",
    "            if self.safe_position(future_position)\n",
    "        }\n",
    "  \n",
    "        if self.previous_action!=None:\n",
    "            actions.pop(opposite(self.previous_action), None)\n",
    "        if any(actions):\n",
    "            action = min(actions.items(), key=lambda x: x[1])[0]\n",
    "            self.previous_action = action\n",
    "            if self.DEBUG:\n",
    "                aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n",
    "                print(f'{id(self)}({self.step}): Greedy_ar_move {action.name} to {aux_pos}', flush=True)\n",
    "            self.previous_action = action\n",
    "            return action.name\n",
    "        else:\n",
    "            return self.strategy_random_avoid_collision(observation, configuration)\n",
    "    \n",
    "    \n",
    "    #Redefine this method\n",
    "    def agent_strategy(self, observation, configuration):\n",
    "        action = self.strategy_greedy_avoid_risk(observation, configuration)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def agent_do(self, observation, configuration):\n",
    "        self.preprocess_env(observation, configuration)\n",
    "        move = self.agent_strategy(observation, configuration)\n",
    "        self.step += 1\n",
    "        #if self.DEBUG:\n",
    "        #    aux_pos = self._translate(self.my_head, self.previous_action), self._row_col(self._translate(self.my_head, self.previous_action))\n",
    "        #    print(f'{id(self)}({self.step}): Move {move} to {aux_pos} internal_vars->{vars(self)}', flush=True)\n",
    "        return move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-02-28T13:41:35.841742Z",
     "iopub.status.busy": "2021-02-28T13:41:35.840601Z",
     "iopub.status.idle": "2021-02-28T13:41:35.844533Z",
     "shell.execute_reply": "2021-02-28T13:41:35.843939Z"
    },
    "papermill": {
     "duration": 0.02236,
     "end_time": "2021-02-28T13:41:35.844683",
     "exception": false,
     "start_time": "2021-02-28T13:41:35.822323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing greedy-goose.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile greedy-goose.py\n",
    "from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, translate, adjacent_positions, min_distance\n",
    "import random as rand\n",
    "from enum import Enum, auto\n",
    "\n",
    "\n",
    "def opposite(action):\n",
    "    if action == Action.NORTH:\n",
    "        return Action.SOUTH\n",
    "    if action == Action.SOUTH:\n",
    "        return Action.NORTH\n",
    "    if action == Action.EAST:\n",
    "        return Action.WEST\n",
    "    if action == Action.WEST:\n",
    "        return Action.EAST\n",
    "    raise TypeError(str(action) + \" is not a valid Action.\")\n",
    "\n",
    "    \n",
    "\n",
    "#Enconding of cell content to build states from observations\n",
    "class CellState(Enum):\n",
    "    EMPTY = 0\n",
    "    FOOD = auto()\n",
    "    GOOSE = auto()\n",
    "\n",
    "\n",
    "#This class encapsulates mos of the low level Hugry Geese stuff    \n",
    "class BornToNotMedalv2:    \n",
    "    def __init__(self):\n",
    "        self.DEBUG=True\n",
    "        self.rows, self.columns = -1, -1        \n",
    "        self.my_index = -1\n",
    "        self.my_head, self.my_tail = -1, -1\n",
    "        self.geese = []\n",
    "        self.heads = []\n",
    "        self.tails = []\n",
    "        self.food = []\n",
    "        self.cell_states = []\n",
    "        self.actions = [action for action in Action]\n",
    "        self.previous_action = None\n",
    "        self.step = 1\n",
    "\n",
    "        \n",
    "    def _adjacent_positions(self, position):\n",
    "        return adjacent_positions(position, self.columns, self.rows)\n",
    " \n",
    "\n",
    "    def _min_distance_to_food(self, position, food=None):\n",
    "        food = food if food!=None else self.food\n",
    "        return min_distance(position, food, self.columns)\n",
    "\n",
    "    \n",
    "    def _row_col(self, position):\n",
    "        return row_col(position, self.columns)\n",
    "    \n",
    "    \n",
    "    def _translate(self, position, direction):\n",
    "        return translate(position, direction, self.columns, self.rows)\n",
    "        \n",
    "        \n",
    "    def preprocess_env(self, observation, configuration):\n",
    "        observation = Observation(observation)\n",
    "        configuration = Configuration(configuration)\n",
    "        \n",
    "        self.rows, self.columns = configuration.rows, configuration.columns        \n",
    "        self.my_index = observation.index\n",
    "        self.hunger_rate = configuration.hunger_rate\n",
    "        self.min_food = configuration.min_food\n",
    "\n",
    "        self.my_head, self.my_tail = observation.geese[self.my_index][0], observation.geese[self.my_index][-1]        \n",
    "        self.my_body = [pos for pos in observation.geese[self.my_index][1:-1]]\n",
    "\n",
    "        \n",
    "        self.geese = [g for i,g in enumerate(observation.geese) if i!=self.my_index  and len(g) > 0]\n",
    "        self.geese_cells = [pos for g in self.geese for pos in g if len(g) > 0]\n",
    "        \n",
    "        self.occupied = [p for p in self.geese_cells]\n",
    "        self.occupied.extend([p for p in observation.geese[self.my_index]])\n",
    "        \n",
    "        \n",
    "        self.heads = [g[0] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n",
    "        self.bodies = [pos  for i,g in enumerate(observation.geese) for pos in g[1:-1] if i!=self.my_index and len(g) > 2]\n",
    "        self.tails = [g[-1] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 1]\n",
    "        self.food = [f for f in observation.food]\n",
    "        \n",
    "        self.adjacent_to_heads = [pos for head in self.heads for pos in self._adjacent_positions(head)]\n",
    "        self.adjacent_to_bodies = [pos for body in self.bodies for pos in self._adjacent_positions(body)]\n",
    "        self.adjacent_to_tails = [pos for tail in self.tails for pos in self._adjacent_positions(tail)]\n",
    "        self.adjacent_to_geese = self.adjacent_to_heads + self.adjacent_to_bodies\n",
    "        self.danger_zone = self.adjacent_to_geese\n",
    "        \n",
    "        #Cell occupation\n",
    "        self.cell_states = [CellState.EMPTY.value for _ in range(self.rows*self.columns)]\n",
    "        for g in self.geese:\n",
    "            for pos in g:\n",
    "                self.cell_states[pos] = CellState.GOOSE.value\n",
    "        for pos in self.heads:\n",
    "                self.cell_states[pos] = CellState.GOOSE.value\n",
    "        for pos in self.my_body:\n",
    "            self.cell_states[pos] = CellState.GOOSE.value\n",
    "                \n",
    "        #detect dead-ends\n",
    "        self.dead_ends = []\n",
    "        for pos_i,_ in enumerate(self.cell_states):\n",
    "            if self.cell_states[pos_i] != CellState.EMPTY.value:\n",
    "                continue\n",
    "            adjacent = self._adjacent_positions(pos_i)\n",
    "            adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n",
    "            num_blocked = sum(adjacent_states)\n",
    "            if num_blocked>=(CellState.GOOSE.value*3):\n",
    "                self.dead_ends.append(pos_i)\n",
    "        \n",
    "        #check for extended dead-ends\n",
    "        new_dead_ends = [pos for pos in self.dead_ends]\n",
    "        while new_dead_ends!=[]:\n",
    "            for pos in new_dead_ends:\n",
    "                self.cell_states[pos]=CellState.GOOSE.value\n",
    "                self.dead_ends.append(pos)\n",
    "            \n",
    "            new_dead_ends = []\n",
    "            for pos_i,_ in enumerate(self.cell_states):\n",
    "                if self.cell_states[pos_i] != CellState.EMPTY.value:\n",
    "                    continue\n",
    "                adjacent = self._adjacent_positions(pos_i)\n",
    "                adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n",
    "                num_blocked = sum(adjacent_states)\n",
    "                if num_blocked>=(CellState.GOOSE.value*3):\n",
    "                    new_dead_ends.append(pos_i)                                    \n",
    "        \n",
    "                \n",
    "    def strategy_random(self, observation, configuration):\n",
    "        if self.previous_action!=None:\n",
    "            action = rand.choice([action for action in Action if action!=opposite(self.previous_action)])\n",
    "        else:\n",
    "            action = rand.choice([action for action in Action])\n",
    "        self.previous_action = action\n",
    "        return action.name\n",
    "                        \n",
    "                        \n",
    "    def safe_position(self, future_position):\n",
    "        return (future_position not in self.occupied) and (future_position not in self.adjacent_to_heads) and (future_position not in self.dead_ends)\n",
    "    \n",
    "    \n",
    "    def valid_position(self, future_position):\n",
    "        return (future_position not in self.occupied) and (future_position not in self.dead_ends)    \n",
    "\n",
    "    \n",
    "    def free_position(self, future_position):\n",
    "        return (future_position not in self.occupied) \n",
    "    \n",
    "                        \n",
    "    def strategy_random_avoid_collision(self, observation, configuration):\n",
    "        dead_end_cell = False\n",
    "        free_cell = True\n",
    "        actions = [action \n",
    "                   for action in Action \n",
    "                   for future_position in [self._translate(self.my_head, action)]\n",
    "                   if self.valid_position(future_position)] \n",
    "        if self.previous_action!=None:\n",
    "            actions = [action for action in actions if action!=opposite(self.previous_action)] \n",
    "        if actions==[]:\n",
    "            dead_end_cell = True\n",
    "            actions = [action \n",
    "                       for action in Action \n",
    "                       for future_position in [self._translate(self.my_head, action)]\n",
    "                       if self.free_position(future_position)]\n",
    "            if self.previous_action!=None:\n",
    "                actions = [action for action in actions if action!=opposite(self.previous_action)] \n",
    "            #no alternatives\n",
    "            if actions==[]:\n",
    "                free_cell = False\n",
    "                actions = self.actions if self.previous_action==None else [action for action in self.actions if action!=opposite(self.previous_action)] \n",
    "\n",
    "        action = rand.choice(actions)\n",
    "        self.previous_action = action\n",
    "        if self.DEBUG:\n",
    "            aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n",
    "            dead_ends = \"\" if not dead_end_cell else f', dead_ends={[self._row_col(p1) for p1 in self.dead_ends]}, occupied={[self._row_col(p2) for p2 in self.occupied]}'\n",
    "            if free_cell:\n",
    "                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} dead_end={dead_end_cell}{dead_ends}', flush=True)\n",
    "            else:\n",
    "                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} free_cell={free_cell}', flush=True)\n",
    "        return action.name\n",
    "    \n",
    "    \n",
    "    def strategy_greedy_avoid_risk(self, observation, configuration):        \n",
    "        actions = {  \n",
    "            action: self._min_distance_to_food(future_position)\n",
    "            for action in Action \n",
    "            for future_position in [self._translate(self.my_head, action)]\n",
    "            if self.safe_position(future_position)\n",
    "        }\n",
    "  \n",
    "        if self.previous_action!=None:\n",
    "            actions.pop(opposite(self.previous_action), None)\n",
    "        if any(actions):\n",
    "            action = min(actions.items(), key=lambda x: x[1])[0]\n",
    "            self.previous_action = action\n",
    "            if self.DEBUG:\n",
    "                aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n",
    "                print(f'{id(self)}({self.step}): Greedy_ar_move {action.name} to {aux_pos}', flush=True)\n",
    "            self.previous_action = action\n",
    "            return action.name\n",
    "        else:\n",
    "            return self.strategy_random_avoid_collision(observation, configuration)\n",
    "    \n",
    "    \n",
    "    #Redefine this method\n",
    "    def agent_strategy(self, observation, configuration):\n",
    "        action = self.strategy_greedy_avoid_risk(observation, configuration)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def agent_do(self, observation, configuration):\n",
    "        self.preprocess_env(observation, configuration)\n",
    "        move = self.agent_strategy(observation, configuration)\n",
    "        self.step += 1\n",
    "        #if self.DEBUG:\n",
    "        #    aux_pos = self._translate(self.my_head, self.previous_action), self._row_col(self._translate(self.my_head, self.previous_action))\n",
    "        #    print(f'{id(self)}({self.step}): Move {move} to {aux_pos} internal_vars->{vars(self)}', flush=True)\n",
    "        return move\n",
    "\n",
    "    \n",
    "    \n",
    "def agent_singleton(observation, configuration):\n",
    "    global gus    \n",
    "    \n",
    "    try:\n",
    "        gus\n",
    "    except NameError:\n",
    "        gus = BornToNotMedalv2()\n",
    "            \n",
    "    action = gus.agent_do(observation, configuration)\n",
    "\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.011352,
     "end_time": "2021-02-28T13:41:35.868010",
     "exception": false,
     "start_time": "2021-02-28T13:41:35.856658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Our Agent class\n",
    "\n",
    "Most of the work is done, all that's left is conecting both base classes, creating the state-space, defining transitions and rewards\n",
    "\n",
    "\n",
    "### State definition\n",
    "\n",
    "Our state will be build from a window of cells centered around our agent head \"seeing\" POV_DISTANCE=2 cells in each direction. This square has total side (POV_DISTANCE*2)+1.\n",
    "\n",
    "We code the contents of the cells using TileState class and use join to build the current state string by concatenating. E.g. with POV_DISTANCE=1:\n",
    "* The state string = \"000020000\"\n",
    "    * Comes from the following cell contents (agents sees only its head)\n",
    "        * 000\n",
    "        * 020\n",
    "        * 000\n",
    "* The state string = \"010020020\"\n",
    "    * Comes from the following cell contents (agents is looking upward an there's food NORTH)\n",
    "        * 010\n",
    "        * 020\n",
    "        * 020\n",
    "        \n",
    "* NOTE: Here we make no diffenrentiation between my body and other agent bodies (states are the minimum to avoid exponential growth of search space).\n",
    "\n",
    "\n",
    "\n",
    "### Positive Rewards\n",
    "\n",
    "* Natural positive reward for eating food (growing)\n",
    "* Postive reward for going towards food (to help guide the learning \"eating is good\")\n",
    "\n",
    "### Common Sense knowledge rewards (penalties)\n",
    "\n",
    "After the first tests I've decided to add somo common sense knowledge to help guide the learning.\n",
    "\n",
    "* Colliding is bad, on move that crashes against geese process a reward of -1 and choose other random, non-colliding move\n",
    "* Same for forbidden moves\n",
    "* Same for avoiding dead-end cells (3 adjacent cells are \"blocked\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-28T13:41:35.908621Z",
     "iopub.status.busy": "2021-02-28T13:41:35.905736Z",
     "iopub.status.idle": "2021-02-28T13:41:35.911677Z",
     "shell.execute_reply": "2021-02-28T13:41:35.910951Z"
    },
    "papermill": {
     "duration": 0.031878,
     "end_time": "2021-02-28T13:41:35.911826",
     "exception": false,
     "start_time": "2021-02-28T13:41:35.879948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from enum import Enum, auto\n",
    "import pickle\n",
    "\n",
    "#Enconding of cell content to build states from observations\n",
    "class CellState(Enum):\n",
    "    EMPTY = 0\n",
    "    FOOD = auto()\n",
    "    GOOSE = auto()\n",
    "    #search space gets too big too fast... so just 3 cell states\n",
    "    #HEAD = auto()\n",
    "    #BODY = auto()\n",
    "    #TAIL = auto()\n",
    "\n",
    "    \n",
    "#This is our Q-Learning Hungry Geese Agent\n",
    "class QGoose(BornToNotMedalv2, QLearner):    \n",
    "    def __init__(self):\n",
    "        self.POV_DISTANCE=3\n",
    "        BornToNotMedalv2.__init__(self)\n",
    "        QLearner.__init__(self, self.actions, initial_value=0.01, alpha=0.1, gamma=.8, epsilon=0.1)\n",
    "        self.world = None\n",
    "        self.previous_length = 0\n",
    "        self.last_min_distance_to_food = self.rows*self.columns #initial max value to mark no food seen so far\n",
    "       \n",
    "    \n",
    "    def title_state_from_row_col(self, row, col):\n",
    "        pos = self.columns*row+col-1\n",
    "        if pos in self.heads or pos==self.my_head:\n",
    "            return CellState.GOOSE\n",
    "        elif pos in self.bodies or pos==self.my_body:\n",
    "            return CellState.GOOSE\n",
    "        elif pos in self.tails or pos==self.my_tail:\n",
    "            return CellState.GOOSE\n",
    "        elif pos in self.food:\n",
    "            return CellState.FOOD\n",
    "        else:\n",
    "            return CellState.EMPTY\n",
    "    \n",
    "    \n",
    "    def state_from_world(self):\n",
    "        state = []\n",
    "        row_0, col_0 = self._row_col(self.my_head)\n",
    "        for col_delta in range (-self.POV_DISTANCE, self.POV_DISTANCE):\n",
    "            for row_delta in range (-self.POV_DISTANCE, self.POV_DISTANCE):\n",
    "                row_i = (row_0+row_delta)%self.rows\n",
    "                col_i = (col_0+col_delta)%self.columns\n",
    "                state.append(self.title_state_from_row_col(row_i, col_i))\n",
    "        state = \"\".join([str(s.value) for s in state])\n",
    "        return state\n",
    "    \n",
    "    \n",
    "    def common_sense_after_move_choosen(self, action):\n",
    "        future_position = self._translate(self.my_head, action)\n",
    "\n",
    "        if future_position in self.occupied:\n",
    "            return -10 \n",
    "        elif self.previous_action==opposite(self.last_action): #opposite is currently a patch until Action.opposite works...\n",
    "            return -10\n",
    "        elif self.previous_action in self.dead_ends:\n",
    "            return -1\n",
    "        else:\n",
    "            min_distance_to_food = self._min_distance_to_food(future_position)\n",
    "            aux_last = self.last_min_distance_to_food\n",
    "            self.last_min_distance_to_food=min_distance_to_food\n",
    "            \n",
    "            if min_distance_to_food<aux_last:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    \n",
    "    \n",
    "    def agent_strategy(self, observation, configuration):\n",
    "        state = self.state_from_world()\n",
    "        \n",
    "        #Process reward for growing\n",
    "        reward = len(self.my_body)+2*self.step #Geese really like pizza!!! 8-)\n",
    "        self.previous_length = len(self.my_body)\n",
    "        self.process_reward(reward)\n",
    "        \n",
    "        #Choose action\n",
    "        action = self.epsilon_greedy_choose_action(state)\n",
    "        \n",
    "        #Apply some common sense like colliding is bad... ;-)\n",
    "        cs_reward = self.common_sense_after_move_choosen(action)\n",
    "        if cs_reward<0:\n",
    "            #update q-table\n",
    "            self.process_reward(reward, previous_state=state, last_action=action, last_action_index=self.actions.index(action))\n",
    "\n",
    "            #choose new greedy risk averse valid action\n",
    "            random_action = self.strategy_greedy_avoid_risk(observation, configuration)                                   \n",
    "            #update internal action attributes\n",
    "            aux = [(action,index) for index,action in enumerate(Action) if action.name==random_action][0]\n",
    "            self.last_action = aux[0]\n",
    "            self.last_action_index = aux[1]\n",
    "            action = self.last_action\n",
    "        print(f'q-agent q_table{self.q_table}', flush=True)\n",
    "        \n",
    "        self.previous_action = action    \n",
    "        return Action(action).name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012289,
     "end_time": "2021-02-28T13:41:35.936297",
     "exception": false,
     "start_time": "2021-02-28T13:41:35.924008",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Now all together\n",
    "\n",
    "We write all clases to 'qgoose.py' (see file for full source code).\n",
    "\n",
    "To learn between different matches here on the notebook I just read a pickle at the start of the game (if it exists) and save back to disk after each update (not very efficient, I know). See \"agent_singleton\" function at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-02-28T13:41:35.969744Z",
     "iopub.status.busy": "2021-02-28T13:41:35.968799Z",
     "iopub.status.idle": "2021-02-28T13:41:35.973057Z",
     "shell.execute_reply": "2021-02-28T13:41:35.973515Z"
    },
    "papermill": {
     "duration": 0.025274,
     "end_time": "2021-02-28T13:41:35.973692",
     "exception": false,
     "start_time": "2021-02-28T13:41:35.948418",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing qgoose.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile qgoose.py\n",
    "\n",
    "from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, translate, adjacent_positions, min_distance\n",
    "import random as rand\n",
    "from enum import Enum, auto\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def opposite(action):\n",
    "    if action == Action.NORTH:\n",
    "        return Action.SOUTH\n",
    "    if action == Action.SOUTH:\n",
    "        return Action.NORTH\n",
    "    if action == Action.EAST:\n",
    "        return Action.WEST\n",
    "    if action == Action.WEST:\n",
    "        return Action.EAST\n",
    "    raise TypeError(str(action) + \" is not a valid Action.\")\n",
    "    \n",
    "\n",
    "    \n",
    "#Enconding of cell content to build states from observations\n",
    "class CellState(Enum):\n",
    "    EMPTY = 0\n",
    "    FOOD = auto()\n",
    "    GOOSE = auto()\n",
    "    #search space gets too big too fast... so just 3 cell states\n",
    "    #HEAD = auto()\n",
    "    #BODY = auto()\n",
    "    #TAIL = auto()\n",
    "    \n",
    "    \n",
    "\n",
    "#This class encapsulates a simple qlearning with epsilon-greedy policy, the states and transitions can be defined automatically as we explore (search space is too big to initialize all at once and many states won't be achievable)\n",
    "class QLearner():\n",
    "    def __init__(self, actions, states=None, initial_value=0.1, alpha=0.3, gamma=0.1, epsilon=0.9, create_states_on_exploration=True):\n",
    "        self.actions = actions\n",
    "        self.create_states_on_exploration = create_states_on_exploration\n",
    "        self.initial_value = initial_value\n",
    "        if states!=None:\n",
    "            self.q_table = {\n",
    "                state: [initial_value for _ in self.actions] for state in states\n",
    "            }\n",
    "            self.states = states\n",
    "        else:\n",
    "            self.q_table = dict()\n",
    "            self.states = []\n",
    "            \n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.previous_state = None\n",
    "        self.current_state = None\n",
    "        self.last_action = None\n",
    "        self.last_action_index = None\n",
    "\n",
    "        \n",
    "    def _check_auto_init_state(self, state):\n",
    "        if (state!=None) and (state not in self.q_table.keys()) and self.create_states_on_exploration:\n",
    "            self.q_table[state] = [self.initial_value for _ in self.actions]\n",
    "            self.states.append(state)\n",
    "    \n",
    "\n",
    "    def _epsilon_greedy(self, state):\n",
    "        #create state if needed\n",
    "        self._check_auto_init_state(state)\n",
    "        \n",
    "        if (rand.random() < self.epsilon):\n",
    "            action = rand.choice(self.actions)\n",
    "            self.last_action_index = self.actions.index(action)\n",
    "        else:\n",
    "            q_state = self.q_table[state]\n",
    "            max_val = max(q_state)\n",
    "            self.last_action_index = rand.choice([i for i,v in enumerate(q_state) if v==max_val])\n",
    "            action = self.actions[self.last_action_index]\n",
    "        return action\n",
    "\n",
    "    \n",
    "    def process_reward(self, reward, previous_state=None, last_action=None, last_action_index=None):\n",
    "        if previous_state==None:\n",
    "            previous_state = self.previous_state\n",
    "        if last_action==None:\n",
    "            last_action = self.last_action\n",
    "        if last_action_index==None:\n",
    "            last_action_index = self.last_action_index\n",
    "            \n",
    "        if (previous_state==None) or (last_action==None):\n",
    "            return\n",
    "        \n",
    "        #create state if needed\n",
    "        self._check_auto_init_state(previous_state)\n",
    "        \n",
    "        q = self.q_table\n",
    "        q_old = q[previous_state][last_action_index]\n",
    "        next_state = self.current_state\n",
    "        if next_state!=None:        \n",
    "            best_scenario = q[next_state].index(max(q[next_state]))\n",
    "            q[previous_state][last_action_index] = q_old + self.alpha * (reward + self.gamma * best_scenario - q_old)\n",
    "        else:\n",
    "            q[previous_state][last_action_index] = q_old + self.alpha * (reward + self.initial_value - q_old)\n",
    "\n",
    "            \n",
    "    def epsilon_greedy_choose_action(self, state):\n",
    "        self.previous_state = self.current_state\n",
    "        self.current_state = state\n",
    "        self.last_action = self._epsilon_greedy(state)\n",
    "        return self.last_action\n",
    "    \n",
    "    \n",
    "    def reset_internal_states(self):\n",
    "        self.previous_state = None\n",
    "        self.current_state = None\n",
    "        self.last_action = None\n",
    "        self.last_action_index = None\n",
    "          \n",
    "            \n",
    "    def save_pickle(self, name):\n",
    "        save_data = (self.actions,\n",
    "                     self.q_table,\n",
    "                     self.states,\n",
    "                     )\n",
    "        with open(f'{name}', 'wb') as handle:\n",
    "            pickle.dump(save_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            \n",
    "    def load_pickle(self, name):\n",
    "        with open(f'{name}', 'rb') as handle:\n",
    "            data = pickle.load(handle)\n",
    "            self.actions, self.q_table, self.states = data\n",
    "            \n",
    "            \n",
    "\n",
    "#This class encapsulates mos of the low level Hugry Geese stuff    \n",
    "#This class encapsulates mos of the low level Hugry Geese stuff    \n",
    "class BornToNotMedalv2:    \n",
    "    def __init__(self):\n",
    "        self.DEBUG=True\n",
    "        self.rows, self.columns = -1, -1        \n",
    "        self.my_index = -1\n",
    "        self.my_head, self.my_tail = -1, -1\n",
    "        self.geese = []\n",
    "        self.heads = []\n",
    "        self.tails = []\n",
    "        self.food = []\n",
    "        self.cell_states = []\n",
    "        self.actions = [action for action in Action]\n",
    "        self.previous_action = None\n",
    "        self.step = 1\n",
    "\n",
    "        \n",
    "    def _adjacent_positions(self, position):\n",
    "        return adjacent_positions(position, self.columns, self.rows)\n",
    " \n",
    "\n",
    "    def _min_distance_to_food(self, position, food=None):\n",
    "        food = food if food!=None else self.food\n",
    "        return min_distance(position, food, self.columns)\n",
    "\n",
    "    \n",
    "    def _row_col(self, position):\n",
    "        return row_col(position, self.columns)\n",
    "    \n",
    "    \n",
    "    def _translate(self, position, direction):\n",
    "        return translate(position, direction, self.columns, self.rows)\n",
    "        \n",
    "        \n",
    "    def preprocess_env(self, observation, configuration):\n",
    "        observation = Observation(observation)\n",
    "        configuration = Configuration(configuration)\n",
    "        \n",
    "        self.rows, self.columns = configuration.rows, configuration.columns        \n",
    "        self.my_index = observation.index\n",
    "        self.hunger_rate = configuration.hunger_rate\n",
    "        self.min_food = configuration.min_food\n",
    "\n",
    "        self.my_head, self.my_tail = observation.geese[self.my_index][0], observation.geese[self.my_index][-1]        \n",
    "        self.my_body = [pos for pos in observation.geese[self.my_index][1:-1]]\n",
    "\n",
    "        \n",
    "        self.geese = [g for i,g in enumerate(observation.geese) if i!=self.my_index  and len(g) > 0]\n",
    "        self.geese_cells = [pos for g in self.geese for pos in g if len(g) > 0]\n",
    "        \n",
    "        self.occupied = [p for p in self.geese_cells]\n",
    "        self.occupied.extend([p for p in observation.geese[self.my_index]])\n",
    "        \n",
    "        \n",
    "        self.heads = [g[0] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 0]\n",
    "        self.bodies = [pos  for i,g in enumerate(observation.geese) for pos in g[1:-1] if i!=self.my_index and len(g) > 2]\n",
    "        self.tails = [g[-1] for i,g in enumerate(observation.geese) if i!=self.my_index and len(g) > 1]\n",
    "        self.food = [f for f in observation.food]\n",
    "        \n",
    "        self.adjacent_to_heads = [pos for head in self.heads for pos in self._adjacent_positions(head)]\n",
    "        self.adjacent_to_bodies = [pos for body in self.bodies for pos in self._adjacent_positions(body)]\n",
    "        self.adjacent_to_tails = [pos for tail in self.tails for pos in self._adjacent_positions(tail)]\n",
    "        self.adjacent_to_geese = self.adjacent_to_heads + self.adjacent_to_bodies\n",
    "        self.danger_zone = self.adjacent_to_geese\n",
    "        \n",
    "        #Cell occupation\n",
    "        self.cell_states = [CellState.EMPTY.value for _ in range(self.rows*self.columns)]\n",
    "        for g in self.geese:\n",
    "            for pos in g:\n",
    "                self.cell_states[pos] = CellState.GOOSE.value\n",
    "        for pos in self.heads:\n",
    "                self.cell_states[pos] = CellState.GOOSE.value\n",
    "        for pos in self.my_body:\n",
    "            self.cell_states[pos] = CellState.GOOSE.value\n",
    "                \n",
    "        #detect dead-ends\n",
    "        self.dead_ends = []\n",
    "        for pos_i,_ in enumerate(self.cell_states):\n",
    "            if self.cell_states[pos_i] != CellState.EMPTY.value:\n",
    "                continue\n",
    "            adjacent = self._adjacent_positions(pos_i)\n",
    "            adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n",
    "            num_blocked = sum(adjacent_states)\n",
    "            if num_blocked>=(CellState.GOOSE.value*3):\n",
    "                self.dead_ends.append(pos_i)\n",
    "        \n",
    "        #check for extended dead-ends\n",
    "        new_dead_ends = [pos for pos in self.dead_ends]\n",
    "        while new_dead_ends!=[]:\n",
    "            for pos in new_dead_ends:\n",
    "                self.cell_states[pos]=CellState.GOOSE.value\n",
    "                self.dead_ends.append(pos)\n",
    "            \n",
    "            new_dead_ends = []\n",
    "            for pos_i,_ in enumerate(self.cell_states):\n",
    "                if self.cell_states[pos_i] != CellState.EMPTY.value:\n",
    "                    continue\n",
    "                adjacent = self._adjacent_positions(pos_i)\n",
    "                adjacent_states = [self.cell_states[adj_pos] for adj_pos in adjacent if adj_pos!=self.my_head]\n",
    "                num_blocked = sum(adjacent_states)\n",
    "                if num_blocked>=(CellState.GOOSE.value*3):\n",
    "                    new_dead_ends.append(pos_i)                                    \n",
    "        \n",
    "                \n",
    "    def strategy_random(self, observation, configuration):\n",
    "        if self.previous_action!=None:\n",
    "            action = rand.choice([action for action in Action if action!=opposite(self.previous_action)])\n",
    "        else:\n",
    "            action = rand.choice([action for action in Action])\n",
    "        self.previous_action = action\n",
    "        return action.name\n",
    "                        \n",
    "                        \n",
    "    def safe_position(self, future_position):\n",
    "        return (future_position not in self.occupied) and (future_position not in self.adjacent_to_heads) and (future_position not in self.dead_ends)\n",
    "    \n",
    "    \n",
    "    def valid_position(self, future_position):\n",
    "        return (future_position not in self.occupied) and (future_position not in self.dead_ends)    \n",
    "\n",
    "    \n",
    "    def free_position(self, future_position):\n",
    "        return (future_position not in self.occupied) \n",
    "    \n",
    "                        \n",
    "    def strategy_random_avoid_collision(self, observation, configuration):\n",
    "        dead_end_cell = False\n",
    "        free_cell = True\n",
    "        actions = [action \n",
    "                   for action in Action \n",
    "                   for future_position in [self._translate(self.my_head, action)]\n",
    "                   if self.valid_position(future_position)] \n",
    "        if self.previous_action!=None:\n",
    "            actions = [action for action in actions if action!=opposite(self.previous_action)] \n",
    "        if actions==[]:\n",
    "            dead_end_cell = True\n",
    "            actions = [action \n",
    "                       for action in Action \n",
    "                       for future_position in [self._translate(self.my_head, action)]\n",
    "                       if self.free_position(future_position)]\n",
    "            if self.previous_action!=None:\n",
    "                actions = [action for action in actions if action!=opposite(self.previous_action)] \n",
    "            #no alternatives\n",
    "            if actions==[]:\n",
    "                free_cell = False\n",
    "                actions = self.actions if self.previous_action==None else [action for action in self.actions if action!=opposite(self.previous_action)] \n",
    "\n",
    "        action = rand.choice(actions)\n",
    "        self.previous_action = action\n",
    "        if self.DEBUG:\n",
    "            aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n",
    "            dead_ends = \"\" if not dead_end_cell else f', dead_ends={[self._row_col(p1) for p1 in self.dead_ends]}, occupied={[self._row_col(p2) for p2 in self.occupied]}'\n",
    "            if free_cell:\n",
    "                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} dead_end={dead_end_cell}{dead_ends}', flush=True)\n",
    "            else:\n",
    "                print(f'{id(self)}({self.step}): Random_ac_move {action.name} to {aux_pos} free_cell={free_cell}', flush=True)\n",
    "        return action.name\n",
    "    \n",
    "    \n",
    "    def strategy_greedy_avoid_risk(self, observation, configuration):        \n",
    "        actions = {  \n",
    "            action: self._min_distance_to_food(future_position)\n",
    "            for action in Action \n",
    "            for future_position in [self._translate(self.my_head, action)]\n",
    "            if self.safe_position(future_position)\n",
    "        }\n",
    "  \n",
    "        if self.previous_action!=None:\n",
    "            actions.pop(opposite(self.previous_action), None)\n",
    "        if any(actions):\n",
    "            action = min(actions.items(), key=lambda x: x[1])[0]\n",
    "            self.previous_action = action\n",
    "            if self.DEBUG:\n",
    "                aux_pos = self._row_col(self._translate(self.my_head, self.previous_action))\n",
    "                print(f'{id(self)}({self.step}): Greedy_ar_move {action.name} to {aux_pos}', flush=True)\n",
    "            self.previous_action = action\n",
    "            return action.name\n",
    "        else:\n",
    "            return self.strategy_random_avoid_collision(observation, configuration)\n",
    "    \n",
    "    \n",
    "    #Redefine this method\n",
    "    def agent_strategy(self, observation, configuration):\n",
    "        action = self.strategy_greedy_avoid_risk(observation, configuration)\n",
    "        return action\n",
    "    \n",
    "    \n",
    "    def agent_do(self, observation, configuration):\n",
    "        self.preprocess_env(observation, configuration)\n",
    "        move = self.agent_strategy(observation, configuration)\n",
    "        self.step += 1\n",
    "        #if self.DEBUG:\n",
    "        #    aux_pos = self._translate(self.my_head, self.previous_action), self._row_col(self._translate(self.my_head, self.previous_action))\n",
    "        #    print(f'{id(self)}({self.step}): Move {move} to {aux_pos} internal_vars->{vars(self)}', flush=True)\n",
    "        return move\n",
    "\n",
    "\n",
    "        \n",
    "#This is our Q-Learning Hungry Geese Agent\n",
    "class QGoose(BornToNotMedalv2, QLearner):    \n",
    "    def __init__(self):\n",
    "        self.POV_DISTANCE=3\n",
    "        BornToNotMedalv2.__init__(self)\n",
    "        QLearner.__init__(self, self.actions, initial_value=0.01, alpha=0.1, gamma=.8, epsilon=0.1)\n",
    "        self.world = None\n",
    "        self.previous_length = 0\n",
    "        self.last_min_distance_to_food = self.rows*self.columns #initial max value to mark no food seen so far\n",
    "       \n",
    "    \n",
    "    def title_state_from_row_col(self, row, col):\n",
    "        pos = self.columns*row+col-1\n",
    "        if pos in self.heads or pos==self.my_head:\n",
    "            return CellState.GOOSE\n",
    "        elif pos in self.bodies or pos==self.my_body:\n",
    "            return CellState.GOOSE\n",
    "        elif pos in self.tails or pos==self.my_tail:\n",
    "            return CellState.GOOSE\n",
    "        elif pos in self.food:\n",
    "            return CellState.FOOD\n",
    "        else:\n",
    "            return CellState.EMPTY\n",
    "    \n",
    "    \n",
    "    def state_from_world(self):\n",
    "        state = []\n",
    "        row_0, col_0 = self._row_col(self.my_head)\n",
    "        for col_delta in range (-self.POV_DISTANCE, self.POV_DISTANCE):\n",
    "            for row_delta in range (-self.POV_DISTANCE, self.POV_DISTANCE):\n",
    "                row_i = (row_0+row_delta)%self.rows\n",
    "                col_i = (col_0+col_delta)%self.columns\n",
    "                state.append(self.title_state_from_row_col(row_i, col_i))\n",
    "        state = \"\".join([str(s.value) for s in state])\n",
    "        return state\n",
    "    \n",
    "    \n",
    "    def common_sense_after_move_choosen(self, action):\n",
    "        future_position = self._translate(self.my_head, action)\n",
    "\n",
    "        if future_position in self.occupied:\n",
    "            return -10 \n",
    "        elif self.previous_action==opposite(self.last_action): #opposite is currently a patch until Action.opposite works...\n",
    "            return -10\n",
    "        elif self.previous_action in self.dead_ends:\n",
    "            return -1\n",
    "        else:\n",
    "            min_distance_to_food = self._min_distance_to_food(future_position)\n",
    "            aux_last = self.last_min_distance_to_food\n",
    "            self.last_min_distance_to_food=min_distance_to_food\n",
    "            \n",
    "            if min_distance_to_food<aux_last:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "    \n",
    "    \n",
    "    def agent_strategy(self, observation, configuration):\n",
    "        state = self.state_from_world()\n",
    "        \n",
    "        #Process reward for growing\n",
    "        reward = len(self.my_body)+2*self.step #Geese really like pizza!!! 8-)\n",
    "        self.previous_length = len(self.my_body)\n",
    "        self.process_reward(reward)\n",
    "        \n",
    "        #Choose action\n",
    "        action = self.epsilon_greedy_choose_action(state)\n",
    "        \n",
    "        #Apply some common sense like colliding is bad... ;-)\n",
    "        cs_reward = self.common_sense_after_move_choosen(action)\n",
    "        if cs_reward<0:\n",
    "            #update q-table\n",
    "            self.process_reward(reward, previous_state=state, last_action=action, last_action_index=self.actions.index(action))\n",
    "\n",
    "            #choose new greedy risk averse valid action\n",
    "            random_action = self.strategy_greedy_avoid_risk(observation, configuration)                                   \n",
    "            #update internal action attributes\n",
    "            aux = [(action,index) for index,action in enumerate(Action) if action.name==random_action][0]\n",
    "            self.last_action = aux[0]\n",
    "            self.last_action_index = aux[1]\n",
    "            action = self.last_action\n",
    "        print(f'q-agent q_table{self.q_table}', flush=True)\n",
    "        \n",
    "        self.previous_action = action    \n",
    "        return Action(action).name\n",
    "\n",
    "\n",
    "        \n",
    "def agent_singleton(observation, configuration):\n",
    "    global gus    \n",
    "    saved=\"qgoose.pickle\"\n",
    "    \n",
    "    try:\n",
    "        gus\n",
    "    except NameError:\n",
    "        gus = QGoose()\n",
    "        if os.path.isfile(saved) and os.stat(saved).st_size>0:\n",
    "            if gus.DEBUG:\n",
    "                print(\"Loading agent, q-table...\")\n",
    "            gus.load_pickle(saved)\n",
    "            if gus.DEBUG:\n",
    "                print(\"Loaded!\")\n",
    "        elif gus.DEBUG:\n",
    "            print(\"No previous trained QTable found!\")\n",
    "            \n",
    "    action = gus.agent_do(observation, configuration)\n",
    "    #print(\"Saving QGoose pickle!!!\", saved)\n",
    "    gus.save_pickle(saved)\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012308,
     "end_time": "2021-02-28T13:41:35.998179",
     "exception": false,
     "start_time": "2021-02-28T13:41:35.985871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now just to check it's working. \n",
    "\n",
    "Let the agent play some games and try to learn against greedy, this takes a while... (results not displayed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2021-02-28T13:41:36.029581Z",
     "iopub.status.busy": "2021-02-28T13:41:36.028961Z",
     "iopub.status.idle": "2021-02-28T14:34:41.123636Z",
     "shell.execute_reply": "2021-02-28T14:34:41.124758Z"
    },
    "papermill": {
     "duration": 3185.113963,
     "end_time": "2021-02-28T14:34:41.125707",
     "exception": false,
     "start_time": "2021-02-28T13:41:36.011744",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import kaggle_environments\n",
    "from kaggle_environments import make\n",
    "from tqdm import tqdm\n",
    "\n",
    "GAMES = 500\n",
    "USE_TQDM = False\n",
    "\n",
    "env = make(\"hungry_geese\", debug=False)\n",
    "games = tqdm(range(GAMES)) if USE_TQDM else range(GAMES)\n",
    "for i in games:\n",
    "    env.run([\n",
    "        \"qgoose.py\",\n",
    "        \"greedy-goose.py\", \n",
    "        \"greedy-goose.py\", \n",
    "        \"greedy-goose.py\", \n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012328,
     "end_time": "2021-02-28T14:34:41.171307",
     "exception": false,
     "start_time": "2021-02-28T14:34:41.158979",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "And see what the agent does now... **Has it learned anything?**\n",
    "\n",
    "Let's see results vs greedy-risk-averse goose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-28T14:34:41.220118Z",
     "iopub.status.busy": "2021-02-28T14:34:41.219153Z",
     "iopub.status.idle": "2021-02-28T14:36:30.106621Z",
     "shell.execute_reply": "2021-02-28T14:36:30.107178Z"
    },
    "papermill": {
     "duration": 108.923463,
     "end_time": "2021-02-28T14:36:30.107378",
     "exception": false,
     "start_time": "2021-02-28T14:34:41.183915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[39, 41, 112, 115],\n",
       " [77, 178, 85, 173],\n",
       " [199, 220, 44, 45],\n",
       " [42, 191, 180, 44],\n",
       " [37, 88, 37, 81],\n",
       " [201, 42, 41, 217],\n",
       " [126, 132, 89, 82],\n",
       " [11, 185, 183, 12],\n",
       " [46, 46, 159, 161],\n",
       " [43, 67, 49, 64]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle_environments import evaluate\n",
    "\n",
    "NUM_EPISODES = 10\n",
    "evaluate(\"hungry_geese\",\n",
    "    [\n",
    "        \"qgoose.py\",\n",
    "        \"greedy-goose.py\", \n",
    "        \"greedy-goose.py\", \n",
    "        \"greedy-goose.py\", \n",
    "    ],\n",
    "    num_episodes=NUM_EPISODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.012791,
     "end_time": "2021-02-28T14:36:30.325157",
     "exception": false,
     "start_time": "2021-02-28T14:36:30.312366",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The results aren't SO bad (ok, yeah they are...), maybe keep working! xD\n",
    "\n",
    "### **EDIT:** \n",
    "* After improving base template class, fine tuning a bit more gamma and switching to greedy risk averse fallback on the q-goose results improved a bit :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-28T14:36:30.356628Z",
     "iopub.status.busy": "2021-02-28T14:36:30.355730Z",
     "iopub.status.idle": "2021-02-28T14:36:31.263246Z",
     "shell.execute_reply": "2021-02-28T14:36:31.262752Z"
    },
    "papermill": {
     "duration": 0.925565,
     "end_time": "2021-02-28T14:36:31.263487",
     "exception": false,
     "start_time": "2021-02-28T14:36:30.337922",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%cp qgoose.py main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-28T14:36:31.299853Z",
     "iopub.status.busy": "2021-02-28T14:36:31.294066Z",
     "iopub.status.idle": "2021-02-28T14:36:32.298367Z",
     "shell.execute_reply": "2021-02-28T14:36:32.297770Z"
    },
    "papermill": {
     "duration": 1.022277,
     "end_time": "2021-02-28T14:36:32.298525",
     "exception": false,
     "start_time": "2021-02-28T14:36:31.276248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "main.py\r\n",
      "qgoose.pickle\r\n"
     ]
    }
   ],
   "source": [
    "!tar cvzf submission.tar.gz main.py qgoose.pickle "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013053,
     "end_time": "2021-02-28T14:36:32.325164",
     "exception": false,
     "start_time": "2021-02-28T14:36:32.312111",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Current conclusions\n",
    "\n",
    "Ok, so far the greedy agent with hard coded rules works better and doesn't need any training time...\n",
    "\n",
    "* The q-learning agent needs to find by itself more common sense knowledge that the other agent has beforehand (move towards food, ...)\n",
    "* Already added some common sense: collision and opposite direction moves negative rewards, collision avoidance\n",
    "* Sometimes greedy just ends crashing with itself\n",
    "* Q-Agent mostly randomly browses :-(\n",
    "\n",
    "### What's next?\n",
    "* Adding information about closest food and/or agents to the state definitions?\n",
    "* Stocastic estimation from current transition to set of \"next states seen from current\"\n",
    "* Tuning hyperparameters\n",
    "* Choosing other technique? xD\n",
    "\n",
    "\n",
    "### Changelog\n",
    "* v52: updated with latest version of greedy risk-averse & dead-end avoid (my base template class)\n",
    "* v46: added reward for getting closer to food and modified the one given when the agent grows\n",
    "* v42: improved negative reward on occupied cells, also added improvements from template base class v31\n",
    "* v40: \n",
    "    * On invalid movement now the q-goose uses greedy risk averse strategy instead of random avoid collision\n",
    "    * Updated to last version of the improved base template (see link below)\n",
    "* Previous versions (no particular order):\n",
    "    * From base template class (See:https://www.kaggle.com/victordelafuente/hungry-geese-template-class-greedy-risk-averse )\n",
    "        * Improved fallback strategies and occupancy accounting\n",
    "        * Fixed couple of bugs to improve default greedy risk averse strategy\n",
    "        * Basic dead-end cell detection\n",
    "    * Higher value of gamma helped getting higher scores ¿seems to work better for this size of search space? (maybe agent needs more time to achieve a direct reward so we take more seriously future reward)\n",
    "\n",
    "\n",
    "### To watch a game\n",
    "\n",
    "Currently doens't save (timeout error on notebook save inside Kaggle) but you can ran the notebook yourself (just uncomment the following block of code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-28T14:36:32.358238Z",
     "iopub.status.busy": "2021-02-28T14:36:32.357390Z",
     "iopub.status.idle": "2021-02-28T14:36:32.362278Z",
     "shell.execute_reply": "2021-02-28T14:36:32.361704Z"
    },
    "papermill": {
     "duration": 0.023954,
     "end_time": "2021-02-28T14:36:32.362444",
     "exception": false,
     "start_time": "2021-02-28T14:36:32.338490",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfrom kaggle_environments import make\\nenv = make(\"hungry_geese\", debug=False)\\nenv.run(\\n    [\\n        \"qgoose.py\", \\n        \"greedy-goose.py\",\\n    ],  \\n)\\nenv.render(mode=\"ipython\", width=800, height=700) #takes too long and hangs firefox many times inside nootebook, no idea why... :-(\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "from kaggle_environments import make\n",
    "env = make(\"hungry_geese\", debug=False)\n",
    "env.run(\n",
    "    [\n",
    "        \"qgoose.py\", \n",
    "        \"greedy-goose.py\",\n",
    "    ],  \n",
    ")\n",
    "env.render(mode=\"ipython\", width=800, height=700) #takes too long and hangs firefox many times inside nootebook, no idea why... :-(\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013164,
     "end_time": "2021-02-28T14:36:32.389204",
     "exception": false,
     "start_time": "2021-02-28T14:36:32.376040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you need to \"reset\" training while on notebook uncomment the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-28T14:36:32.426847Z",
     "iopub.status.busy": "2021-02-28T14:36:32.421036Z",
     "iopub.status.idle": "2021-02-28T14:36:32.442764Z",
     "shell.execute_reply": "2021-02-28T14:36:32.442247Z"
    },
    "papermill": {
     "duration": 0.04013,
     "end_time": "2021-02-28T14:36:32.442911",
     "exception": false,
     "start_time": "2021-02-28T14:36:32.402781",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script bash\n",
    "#rm qgoose.pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013318,
     "end_time": "2021-02-28T14:36:32.470121",
     "exception": false,
     "start_time": "2021-02-28T14:36:32.456803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "*Thanks for checking this out!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3303.593082,
   "end_time": "2021-02-28T14:36:33.803479",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-28T13:41:30.210397",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
